{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Colab test"
      ],
      "metadata": {
        "id": "pENBQJe15bq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# empty for debug/admin"
      ],
      "metadata": {
        "id": "84Yw17iGGPxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This script trains a deep unrolling network for SAR despeckling.\n",
        "# Steps:\n",
        "# 0. Pay for premium GPU :)\n",
        "# 1. Set up dataset (simulated speckle noise on optical images)\n",
        "# 2. Define model\n",
        "# 3. Train with Charbonnier + Total Variation loss\n",
        "# 4. Test on real SAR images/test noisy images"
      ],
      "metadata": {
        "id": "vZ-Z10YNLw4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "import os\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "import gc\n",
        "from tqdm import tqdm  # For progress bar\n",
        "import torch.cuda.memory as cuda_mem"
      ],
      "metadata": {
        "id": "bgwbh5Cr3sjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If you wanna clear cache.. idk sometimes still high RAM :[\n",
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "MsjzHsI-EWDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This block loads and unzip learning dataset from G Drive\n",
        "# Allow all permissions\n",
        "# You should see /content/drive folder in the file browser\n",
        "\n",
        "# Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Select CUDA device (GPU is recommended)\n",
        "# Automatically select CPU if no GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Make folder to put your own test real SAR images\n",
        "folder_name = \"/content/imported_images\"\n",
        "\n",
        "# Create if doesn't exist\n",
        "os.makedirs(folder_name, exist_ok=True)\n",
        "\n",
        "# Make model folder if you want to load your own model\n",
        "os.makedirs(\"/content/model\", exist_ok=True)  # Make a folder exist\n",
        "\n",
        "# drive.mount('/content/drive')  # Connect to your Google Drive\n",
        "\n",
        "\n",
        "# Uncomment below if you want to use validation dataset\n",
        "# # Define paths for zip files\n",
        "# zip_path = \"/content/drive/MyDrive/SAR_Project/Dataset/SAR_paired.zip\"\n",
        "# extract_path = \"/content/test_data/SAR_Dataset\"  # Where to extract\n",
        "\n",
        "\n",
        "# # Step 3: Extract ZIP\n",
        "# print(\"Extracting dataset... this may take a moment!\")\n",
        "# with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "#     zip_ref.extractall(extract_path)\n",
        "\n",
        "# print(\"Extraction complete!\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Zbp1kBQN6Djw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load autoencoder and model"
      ],
      "metadata": {
        "id": "cORdLl_W_EBm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Auto encoder"
      ],
      "metadata": {
        "id": "8ImwA0EJF3GW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # It was initially 512 but I ran out of GPU ram too fast...\n",
        "\n",
        "class ResUNet_256(nn.Module):\n",
        "    def __init__(self, channels=1):\n",
        "        super(ResUNet_256, self).__init__()\n",
        "        # Encoder Path with BatchNorm\n",
        "        self.enc1 = self.conv_block(channels, 64)\n",
        "        self.enc2 = self.conv_block(64, 128)\n",
        "        self.enc3 = self.conv_block(128, 256)\n",
        "        self.enc4 = self.conv_block(256, 256)\n",
        "\n",
        "        # Decoder Path with optimized skip connections\n",
        "        self.dec4 = self.upconv_block(256, 256)\n",
        "        self.dec3 = self.upconv_block(256, 128)\n",
        "        self.dec2 = self.upconv_block(128, 64)\n",
        "        self.dec1 = self.conv_block(64, 32)\n",
        "\n",
        "        # Final Output Layer\n",
        "        self.final_conv = nn.Conv2d(32, 1, kernel_size=3, padding=1)\n",
        "\n",
        "    def conv_block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.1)\n",
        "        )\n",
        "\n",
        "    def upconv_block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=3,\n",
        "                               stride=2, padding=1, output_padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.1)\n",
        "        )\n",
        "\n",
        "    def crop(self, enc_feat, dec_feat):\n",
        "        \"\"\"Crop encoder features to match decoder features spatially.\"\"\"\n",
        "        _, _, h, w = dec_feat.size()\n",
        "        return enc_feat[:, :, :h, :w]\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        e1 = self.enc1(x)\n",
        "        e2 = self.enc2(F.max_pool2d(e1, 2))\n",
        "        e3 = self.enc3(F.max_pool2d(e2, 2))\n",
        "        e4 = self.enc4(F.max_pool2d(e3, 2))\n",
        "\n",
        "        # Decoder with optimized skip connections\n",
        "        d4_temp = self.dec4(e4)\n",
        "        d4 = d4_temp + self.crop(e3, d4_temp)\n",
        "\n",
        "        d3_temp = self.dec3(d4)\n",
        "        d3 = d3_temp + self.crop(e2, d3_temp)\n",
        "\n",
        "        d2_temp = self.dec2(d3)\n",
        "        d2 = d2_temp + self.crop(e1, d2_temp)\n",
        "\n",
        "        d1 = self.dec1(d2)\n",
        "\n",
        "        # Residual subtraction (with scaling)\n",
        "        out = self.final_conv(d1)\n",
        "        return F.relu(out)\n"
      ],
      "metadata": {
        "id": "xp09wmXS1dbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Debug to check resUnet output (was broken a few times before hand)\n",
        "test_input = torch.randn(1, 1, 256, 256)  # Simulated SAR patch\n",
        "model = ResUNet_256()\n",
        "output = model(test_input)\n",
        "\n",
        "print(\"ResUNet Output Shape:\", output.shape)  # Should match input e.g. (1, 1, 512, 512)\n",
        "\n"
      ],
      "metadata": {
        "id": "81B6SFqWMMtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "OH983E78F1d-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uCuFx0C5Z3I"
      },
      "outputs": [],
      "source": [
        "# ========== SAR DeepSpeck model ==========\n",
        "\n",
        "class SAR_DeepSpeck(nn.Module):\n",
        "    def __init__(self, num_layers=8):\n",
        "        super(SAR_DeepSpeck, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.resunet = ResUNet_256()\n",
        "\n",
        "        self.gradient_steps = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(1, 8, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(8),\n",
        "                nn.ReLU(),\n",
        "                nn.Conv2d(8, 1, kernel_size=3, padding=1)\n",
        "            ) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Make δ and η trainable parameters\n",
        "        self.delta = nn.Parameter(torch.tensor(0.01, dtype=torch.float32))  # Init to 0.01\n",
        "        self.eta = nn.Parameter(torch.tensor(1.0, dtype=torch.float32))    # Init to 1.0\n",
        "\n",
        "    def forward(self, x):\n",
        "        v = x  # Initial guess\n",
        "        for i in range(self.num_layers):\n",
        "            noise_est = self.resunet(v)     # Estimate noise at current step\n",
        "            x = x - self.delta * (self.eta * self.gradient_steps[i](noise_est))  # Update x\n",
        "            v = torch.relu(x) # Clamp vlaues to avoid negative value\n",
        "\n",
        "        return x - self.resunet(v)\n",
        "        # return x + v"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load previous model (if any)"
      ],
      "metadata": {
        "id": "GJeIdeb2-n_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##########################\n",
        "# To load other models created before\n",
        "#########################\n",
        "\n",
        "# Put model in this directory: /content/model/\n",
        "\n",
        "# Load the trained SAR-DURNet model\n",
        "model = SAR_DeepSpeck().to(device)\n",
        "\n",
        "# Load the saved model weights\n",
        "checkpoint_path = \"/content/model/SAR_DeepSpeck.pth\"\n",
        "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "print(\"Model successfully loaded!\")"
      ],
      "metadata": {
        "id": "O_WcZ9317pcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test on validation dataset\n",
        "Only if you loaded the validation dataset, else skip to real SAR images"
      ],
      "metadata": {
        "id": "9m8t1pp5-hOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============= To test on validation images ==============================\n",
        "\n",
        "import glob\n",
        "import cv2\n",
        "import torch\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define paths\n",
        "noisy_folder = \"/content/test_data/SAR_Dataset/SAR despeckling filters dataset/Main folder/Noisy_val\"\n",
        "clean_folder = \"/content/test_data/SAR_Dataset/SAR despeckling filters dataset/Main folder/GTruth_val\"\n",
        "output_folder = \"/content/despeckled_val_results\"  # Where despeckled images will be saved\n",
        "\n",
        "# Make sure output folder exists\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Load model in eval mode\n",
        "model.eval()\n",
        "\n",
        "# Load noisy & clean image pairs\n",
        "noisy_images = sorted(glob.glob(noisy_folder + \"/*.tiff\"))\n",
        "clean_images = sorted(glob.glob(clean_folder + \"/*.tiff\"))\n",
        "\n",
        "assert len(noisy_images) == len(clean_images), \"Mismatch in number of noisy and clean images!\"\n",
        "\n",
        "# Store outputs for later visualization\n",
        "results = []\n",
        "\n",
        "# Resize to multiple of 32 for U-Net compatibility\n",
        "def resize_to_multiple_of_32(image):\n",
        "    h, w = image.shape\n",
        "    new_h = (h // 32) * 32\n",
        "    new_w = (w // 32) * 32\n",
        "    return cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "# Process each image\n",
        "for noisy_path, clean_path in zip(noisy_images, clean_images):\n",
        "    # print(f\"Processing {noisy_path}...\")\n",
        "\n",
        "    # Load images\n",
        "    noisy = cv2.imread(noisy_path, cv2.IMREAD_GRAYSCALE) / 255.0\n",
        "    clean = cv2.imread(clean_path, cv2.IMREAD_GRAYSCALE) / 255.0\n",
        "\n",
        "    # Resize\n",
        "    noisy_resized = resize_to_multiple_of_32(noisy)\n",
        "    clean_resized = resize_to_multiple_of_32(clean)\n",
        "\n",
        "    # Convert to tensor\n",
        "    noisy_tensor = torch.tensor(noisy_resized, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
        "\n",
        "    # Run through model\n",
        "    with torch.no_grad():\n",
        "        despeckled = model(noisy_tensor).squeeze().cpu().numpy()\n",
        "    despeckled = np.clip(despeckled, 0, 1)\n",
        "\n",
        "    # Save despeckled image\n",
        "    output_path = os.path.join(output_folder, os.path.basename(noisy_path))\n",
        "    cv2.imwrite(output_path, (despeckled * 255).astype(np.uint8))\n",
        "\n",
        "    # Store for visualization\n",
        "    results.append((noisy_resized, despeckled, clean_resized))\n",
        "\n",
        "print(\"ALL VALIDATION IMAGES PROCESSED!\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T-VaKbmdlj4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show 15 random samples at the end\n",
        "num_samples = min(15, len(results))\n",
        "sample_indices = np.random.choice(len(results), num_samples, replace=False)\n",
        "\n",
        "plt.figure(figsize=(5, 30))\n",
        "for i, idx in enumerate(sample_indices):\n",
        "    noisy, despeckled, clean = results[idx]\n",
        "\n",
        "    plt.subplot(num_samples, 3, i * 3 + 1)\n",
        "    plt.imshow(noisy, cmap=\"gray\")\n",
        "    plt.title(\"Noisy Input\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(num_samples, 3, i * 3 + 2)\n",
        "    plt.imshow(despeckled, cmap=\"gray\")\n",
        "    plt.title(\"Despeckled Output\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(num_samples, 3, i * 3 + 3)\n",
        "    plt.imshow(clean, cmap=\"gray\")\n",
        "    plt.title(\"Ground Truth\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "pXxEbwaBmPai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# zip output folder to download ezpzggwp\n",
        "\n",
        "import shutil\n",
        "shutil.make_archive(\"/content/despeckled_val_results\", 'zip', \"/content/despeckled_val_results\")"
      ],
      "metadata": {
        "id": "i1PDD24FmgV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# To use model on real SAR images"
      ],
      "metadata": {
        "id": "FX-Z0ElI-agj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use model on real SAR images"
      ],
      "metadata": {
        "id": "o5zP48trGeX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============== To test on real SAR images ================================\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def extract_overlapping_patches_with_padding(image, patch_size=256, stride=128):\n",
        "    h, w = image.shape\n",
        "\n",
        "    pad_h = (np.ceil((h - patch_size) / stride) * stride + patch_size - h).astype(int)\n",
        "    pad_w = (np.ceil((w - patch_size) / stride) * stride + patch_size - w).astype(int)\n",
        "\n",
        "    padded_image = np.pad(image, ((0, pad_h), (0, pad_w)), mode='reflect')\n",
        "    padded_h, padded_w = padded_image.shape\n",
        "\n",
        "    patches = []\n",
        "    positions = []\n",
        "\n",
        "    for y in range(0, padded_h - patch_size + 1, stride):\n",
        "        for x in range(0, padded_w - patch_size + 1, stride):\n",
        "            patch = padded_image[y:y+patch_size, x:x+patch_size]\n",
        "            patches.append(patch)\n",
        "            positions.append((y, x))\n",
        "\n",
        "    return patches, positions, padded_image.shape, (h, w)\n",
        "\n",
        "\n",
        "def merge_overlapping_patches(patches, positions, padded_shape, original_shape, patch_size=256):\n",
        "    h_pad, w_pad = padded_shape\n",
        "    h_orig, w_orig = original_shape\n",
        "\n",
        "    recon = np.zeros((h_pad, w_pad), dtype=np.float32)\n",
        "    weight = np.zeros((h_pad, w_pad), dtype=np.float32)\n",
        "\n",
        "    for patch, (y, x) in zip(patches, positions):\n",
        "        recon[y:y+patch_size, x:x+patch_size] += patch\n",
        "        weight[y:y+patch_size, x:x+patch_size] += 1.0\n",
        "\n",
        "    recon /= weight\n",
        "    return recon[:h_orig, :w_orig]  # Crop back to original\n",
        "\n",
        "def create_gaussian_weight(patch_size=256, sigma=0.125):\n",
        "    \"\"\"Create a smooth 2D Gaussian weight mask for blending overlapping patches.\"\"\"\n",
        "    ax = np.linspace(-1, 1, patch_size)\n",
        "    gauss = np.exp(-0.5 * (ax / sigma) ** 2)\n",
        "    weight = np.outer(gauss, gauss)\n",
        "    return weight / weight.max()\n",
        "\n",
        "\n",
        "def merge_patches_soft(patches, positions, padded_shape, original_shape, patch_size=256):\n",
        "    h_pad, w_pad = padded_shape\n",
        "    h_orig, w_orig = original_shape\n",
        "\n",
        "    recon = np.zeros((h_pad, w_pad), dtype=np.float32)\n",
        "    weight_sum = np.zeros((h_pad, w_pad), dtype=np.float32)\n",
        "\n",
        "    weight_mask = create_trimmed_weight(patch_size)  # Soft blending mask\n",
        "\n",
        "    for patch, (y, x) in zip(patches, positions):\n",
        "        weighted_patch = patch * weight_mask\n",
        "        recon[y:y+patch_size, x:x+patch_size] += weighted_patch\n",
        "        weight_sum[y:y+patch_size, x:x+patch_size] += weight_mask\n",
        "\n",
        "    final = recon / (weight_sum + 1e-8)\n",
        "    return final[:h_orig, :w_orig]  # Crop to original size\n",
        "\n",
        "def create_trimmed_weight(patch_size=256, inner_ratio=0.7, sigma=0.3):\n",
        "    ax = np.linspace(-1, 1, patch_size)\n",
        "    gauss = np.exp(-0.5 * (ax / sigma) ** 2)\n",
        "    outer = np.outer(gauss, gauss)\n",
        "    inner_mask = np.zeros_like(outer)\n",
        "\n",
        "    start = int(patch_size * ((1 - inner_ratio) / 2))\n",
        "    end = patch_size - start\n",
        "    inner_mask[start:end, start:end] = 1.0\n",
        "\n",
        "    trimmed = np.maximum(inner_mask, outer)\n",
        "    return trimmed / trimmed.max()\n",
        "\n",
        "\n",
        "# Process a single image through the model using patching\n",
        "def process_image_with_overlap_padded(image_path, model, device, patch_size=256, stride=128):\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE) / 255.0\n",
        "    image = image.astype(np.float32)\n",
        "\n",
        "    patches, positions, padded_shape, original_shape = extract_overlapping_patches_with_padding(\n",
        "        image, patch_size, stride)\n",
        "\n",
        "    output_patches = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for patch in patches:\n",
        "            tensor = torch.tensor(patch).unsqueeze(0).unsqueeze(0).to(device)\n",
        "            out = model(tensor).squeeze().cpu().numpy()\n",
        "            out = np.clip(out, 0, 1)\n",
        "            output_patches.append(out)\n",
        "\n",
        "    despeckled = merge_patches_soft(output_patches, positions, padded_shape, original_shape, patch_size)\n",
        "    return image, despeckled\n",
        "\n",
        "\n",
        "# ========== Run it on all images in your folder ==========\n",
        "\n",
        "input_folder = \"/content/imported_images/\"\n",
        "output_folder = \"/content/despeckled_results_patched/\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "image_paths = sorted([os.path.join(input_folder, f) for f in os.listdir(input_folder) if f.lower().endswith((\".png\", \".jpg\", \".tif\", \".tiff\"))])\n",
        "\n",
        "for img_path in image_paths:\n",
        "    print(f\"Processing {img_path}\")\n",
        "    original, despeckled = process_image_with_overlap_padded(img_path, model, device)\n",
        "\n",
        "\n",
        "    # Save the result\n",
        "    out_name = os.path.basename(img_path)\n",
        "    output_path = os.path.join(output_folder, out_name)\n",
        "    cv2.imwrite(output_path, (despeckled * 255).astype(np.uint8))\n",
        "    print(f\"Saved to {output_path}\")\n",
        "\n",
        "    # Optional: Show comparison\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(original, cmap=\"gray\")\n",
        "    plt.title(\"Original SAR\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(despeckled, cmap=\"gray\")\n",
        "    plt.title(\"Despeckled\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "UEN-F3wT-Q-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# zip output folder to download ezpzggwp\n",
        "\n",
        "import shutil\n",
        "shutil.make_archive(\"/content/despeckled_results_patched\", 'zip', \"/content/despeckled_results_patched\")\n"
      ],
      "metadata": {
        "id": "qKx5tnHAxsrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "fH3ciPrmURcx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}